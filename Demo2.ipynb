{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_lassa_pdfs():\n",
    "    # The base URL that hosts the PDFs\n",
    "    base_url = \"https://ncdc.gov.ng\"\n",
    "    # The specific page that lists all the Lassa fever situation reports\n",
    "    list_page_url = (\n",
    "        \"https://ncdc.gov.ng/diseases/sitreps/?cat=5&name=An%20update%20of%20Lassa%20fever%20outbreak%20in%20Nigeria\"\n",
    "    )\n",
    "    \n",
    "    # Create a local folder to store the PDFs\n",
    "    os.makedirs(\"pdfs\", exist_ok=True)\n",
    "    \n",
    "    # 1. Fetch the HTML\n",
    "    print(f\"Fetching list page: {list_page_url}\")\n",
    "    response = requests.get(list_page_url)\n",
    "    response.raise_for_status()  # raise an error if the HTTP request failed\n",
    "    \n",
    "    # 2. Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # The table is inside <tbody>. Each row has multiple <td>, \n",
    "    # and the third <td> has the <a> with the PDF link\n",
    "    table_body = soup.find(\"tbody\")\n",
    "    if not table_body:\n",
    "        print(\"Could not find <tbody> on the page.\")\n",
    "        return\n",
    "    \n",
    "    rows = table_body.find_all(\"tr\")\n",
    "    if not rows:\n",
    "        print(\"No <tr> found inside <tbody>.\")\n",
    "        return\n",
    "\n",
    "    # For stats\n",
    "    total_found = 0\n",
    "    total_downloaded = 0\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) < 3:\n",
    "            # We expect 3 <td> in each row: (1) index, (2) description, (3) the PDF link\n",
    "            continue\n",
    "        \n",
    "        # The PDF link is in the third cell; let's get the <a>:\n",
    "        link_tag = cells[2].find(\"a\", href=True)\n",
    "        if not link_tag:\n",
    "            continue\n",
    "        \n",
    "        # The PDF URL is relative, e.g. \"/themes/common/files/sitreps/...\"\n",
    "        # We need to prepend https://ncdc.gov.ng\n",
    "        pdf_url = link_tag[\"href\"]\n",
    "        if pdf_url.startswith(\"/\"):\n",
    "            pdf_url = base_url + pdf_url\n",
    "        \n",
    "        # The \"download\" attribute often has the suggested filename\n",
    "        # or we can parse from the final part of the URL\n",
    "        download_name = link_tag.get(\"download\")  # e.g. \"An update of Lassa fever ... .pdf\"\n",
    "        \n",
    "        if not download_name:\n",
    "            # Fallback: parse the filename from the URL\n",
    "            download_name = pdf_url.split(\"/\")[-1]\n",
    "\n",
    "        # Clean up the download name if needed\n",
    "        download_name = download_name.replace(\" \", \"_\")\n",
    "\n",
    "        total_found += 1\n",
    "        # 3. Download the PDF\n",
    "        # We'll skip if it already exists. Or you can overwrite by removing the check.\n",
    "        local_path = os.path.join(\"pdfs\", download_name)\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"Already downloaded: {download_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Downloading {pdf_url} -> {local_path}\")\n",
    "        try:\n",
    "            pdf_response = requests.get(pdf_url)\n",
    "            pdf_response.raise_for_status()\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(pdf_response.content)\n",
    "            total_downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "    print(f\"Found {total_found} PDF links total. Downloaded {total_downloaded} new PDFs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_lassa_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdf_files = os.listdir('PDFs')\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def rename_lassa_files(folder_path):\n",
    "    \"\"\"\n",
    "    Renames 'An_update_of_Lassa_fever_outbreak_in_Nigeria_041124_45.pdf'\n",
    "    to 'Nigeria_04_Nov_24_W45.pdf', extracting day=04, month=11 => 'Nov', year=24,\n",
    "    and the week number 45.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder that contains the PDF files.\n",
    "    \"\"\"\n",
    "    # For mapping month number to short name\n",
    "    month_map = {\n",
    "        \"01\": \"Jan\", \"02\": \"Feb\", \"03\": \"Mar\", \"04\": \"Apr\",\n",
    "        \"05\": \"May\", \"06\": \"Jun\", \"07\": \"Jul\", \"08\": \"Aug\",\n",
    "        \"09\": \"Sep\", \"10\": \"Oct\", \"11\": \"Nov\", \"12\": \"Dec\",\n",
    "    }\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    for file_path in folder.iterdir():\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        if not file_path.suffix.lower() == \".pdf\":\n",
    "            continue\n",
    "        \n",
    "        old_name = file_path.name\n",
    "        # Example old_name: \"An_update_of_Lassa_fever_outbreak_in_Nigeria_041124_45.pdf\"\n",
    "        \n",
    "        # 1) Split on underscores\n",
    "        parts = old_name.split(\"_\")\n",
    "        # e.g. [\"An\",\"update\",\"of\",\"Lassa\",\"fever\",\"outbreak\",\"in\",\"Nigeria\",\"041124\",\"45.pdf\"]\n",
    "        \n",
    "        if len(parts) < 9:\n",
    "            # If the file name doesn't match the expected pattern, skip it\n",
    "            print(f\"Skipping file (unrecognized pattern): {old_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 2) The date chunk is parts[8] like \"041124\"\n",
    "        date_str = parts[8]  # \"041124\"\n",
    "        \n",
    "        # 3) The week chunk is in parts[9], but includes \".pdf\" at the end, e.g. \"45.pdf\"\n",
    "        week_str_pdf = parts[9]  # \"45.pdf\"\n",
    "        # Remove \".pdf\" from the end\n",
    "        if week_str_pdf.endswith(\".pdf\"):\n",
    "            week_str = week_str_pdf.replace(\".pdf\", \"\")\n",
    "        else:\n",
    "            print(f\"Skipping file (no .pdf in last part): {old_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 4) date_str should be 6 characters: DDMMYY\n",
    "        if len(date_str) != 6:\n",
    "            print(f\"Skipping file (date string not 6 chars): {old_name}\")\n",
    "            continue\n",
    "        dd = date_str[0:2]   # \"04\"\n",
    "        mm = date_str[2:4]   # \"11\"\n",
    "        yy = date_str[4:6]   # \"24\"\n",
    "        \n",
    "        # 5) Convert mm => month name\n",
    "        month_name = month_map.get(mm, \"???\" )  # fallback \"???\"\n",
    "        \n",
    "        # 6) Build new name\n",
    "        # e.g. \"Nigeria_04_Nov_24_W45.pdf\"\n",
    "        new_name = f\"Nigeria_{dd}_{month_name}_{yy}_W{week_str}.pdf\"\n",
    "        \n",
    "        new_path = folder / new_name\n",
    "        # 7) Rename the file\n",
    "        print(f\"Renaming:\\n  {old_name}\\n-> {new_name}\\n\")\n",
    "        file_path.rename(new_path)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    rename_lassa_files(\"PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFs for 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted PDFs for 2024: ['Nigeria_04_Jan_24_W1.pdf', 'Nigeria_11_Jan_24_W2.pdf', 'Nigeria_18_Jan_24_W3.pdf', 'Nigeria_25_Jan_24_W4.pdf', 'Nigeria_01_Feb_24_W5.pdf', 'Nigeria_08_Feb_24_W6.pdf', 'Nigeria_15_Feb_24_W7.pdf', 'Nigeria_22_Feb_24_W8.pdf', 'Nigeria_29_Feb_24_W9.pdf', 'Nigeria_07_Mar_24_W10.pdf', 'Nigeria_14_Mar_24_W11.pdf', 'Nigeria_21_Mar_24_W12.pdf', 'Nigeria_28_Mar_24_W13.pdf', 'Nigeria_04_Apr_24_W14.pdf', 'Nigeria_11_Apr_24_W15.pdf', 'Nigeria_18_Apr_24_W16.pdf', 'Nigeria_25_Apr_24_W17.pdf', 'Nigeria_02_May_24_W18.pdf', 'Nigeria_09_May_24_W19.pdf', 'Nigeria_16_May_24_W20.pdf', 'Nigeria_23_May_24_W21.pdf', 'Nigeria_30_May_24_W22.pdf', 'Nigeria_06_Jun_24_W23.pdf', 'Nigeria_13_Jun_24_W24.pdf', 'Nigeria_20_Jun_24_W25.pdf', 'Nigeria_27_Jun_24_W26.pdf', 'Nigeria_04_Jul_24_W27.pdf', 'Nigeria_11_Jul_24_W28.pdf', 'Nigeria_18_Jul_24_W29.pdf', 'Nigeria_25_Jul_24_W30.pdf', 'Nigeria_01_Aug_24_W31.pdf', 'Nigeria_08_Aug_24_W32.pdf', 'Nigeria_15_Aug_24_W33.pdf', 'Nigeria_22_Aug_24_W34.pdf', 'Nigeria_29_Aug_24_W35.pdf', 'Nigeria_05_Sep_24_W36.pdf', 'Nigeria_12_Sep_24_W37.pdf', 'Nigeria_19_Sep_24_W38.pdf', 'Nigeria_26_Sep_24_W39.pdf', 'Nigeria_03_Oct_24_W40.pdf', 'Nigeria_10_Oct_24_W41.pdf', 'Nigeria_14_Oct_24_W42.pdf', 'Nigeria_21_Oct_24_W43.pdf', 'Nigeria_28_Oct_24_W44.pdf', 'Nigeria_04_Nov_24_W45.pdf', 'Nigeria_11_Nov_24_W46.pdf', 'Nigeria_18_Nov_24_W47.pdf', 'Nigeria_25_Nov_24_W48.pdf', 'Nigeria_02_Dec_24_W49.pdf', 'Nigeria_09_Dec_24_W50.pdf', 'Nigeria_16_Dec_24_W51.pdf', 'Nigeria_23_Dec_24_W52.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "all_pdfs = [f for f in os.listdir(\"PDFs\") if f.endswith(\".pdf\")]\n",
    "pdfs_2024 = [f for f in all_pdfs if \"_24_W\" in f]\n",
    "#print(\"PDFs for 2024:\", pdfs_2024)\n",
    "\n",
    "sorted_pdfs = sorted(pdfs_2024, key=lambda x: int(re.search(r'_W(\\d+)\\.pdf$', x).group(1)))\n",
    "print(\"Sorted PDFs for 2024:\", sorted_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in sorted_pdfs:\n",
    "    input_pdf = os.path.join(\"PDFs\", pdf)\n",
    "    output_img = os.path.join(\"PDFs_Lines\", f\"Lines_{pdf.replace('.pdf','')}_page3.png\")\n",
    "    enhance_table_lines_from_pdf_hq(input_pdf, output_img, tr1=700, linelength1=1700,linegap1=100, toler1 = 10 , page_number=3, dpi=300) #length 40 was decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageColor\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def hex_to_hsv(hex_color):\n",
    "    rgb = ImageColor.getcolor(hex_color, \"RGB\")\n",
    "    r, g, b = [x / 255.0 for x in rgb]\n",
    "    hsv = cv2.cvtColor(np.uint8([[[b * 255, g * 255, r * 255]]]), cv2.COLOR_BGR2HSV)[0][0]\n",
    "    return hsv\n",
    "\n",
    "def enhance_table_lines_from_pdf_hq(pdf_path, output_path,tr1, linelength1, linegap1,toler1, page_number=0, dpi=300):\n",
    "    \"\"\"\n",
    "    Enhances vertical column separators and draws horizontal lines at\n",
    "    top boundary, bottom boundary, and header bottom.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF.\n",
    "        output_path (str): Path to save the image (use .png).\n",
    "        page_number (int): Page to process (0-indexed).\n",
    "        dpi (int): Resolution for rendering the PDF page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number]\n",
    "\n",
    "    # 1. Render the PDF page at high DPI\n",
    "    pix = page.get_pixmap(dpi=dpi) \n",
    "    img_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    # Convert PIL Image to OpenCV BGR\n",
    "    img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 2. Convert to HSV to detect green rows\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = hex_to_hsv(\"#D8EDCF\")\n",
    "    tolerance = toler1\n",
    "    lower_green = np.array([max(0, target_hsv[0] - tolerance), 50, 50])\n",
    "    upper_green = np.array([min(179, target_hsv[0] + tolerance), 255, 255])\n",
    "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    h_proj_green = np.sum(green_mask, axis=1)\n",
    "    green_row_indices = np.where(h_proj_green > 50)[0]\n",
    "\n",
    "    if len(green_row_indices) == 0:\n",
    "        print(\"No green rows detected.\")\n",
    "        return\n",
    "\n",
    "    top_boundary = green_row_indices[0]\n",
    "    bottom_boundary = green_row_indices[-1]\n",
    "\n",
    "    # 3. Header Row Detection (just above top_boundary)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    header_region = gray[:top_boundary, :]\n",
    "\n",
    "    # Use Otsu’s threshold instead of fixed 180\n",
    "    _, binary_header = cv2.threshold(header_region, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    h_proj_header = np.sum(binary_header, axis=1)\n",
    "\n",
    "    header_bottom = 0\n",
    "    for i in range(len(h_proj_header) - 1, 0, -1):\n",
    "        if h_proj_header[i] > 40:\n",
    "            header_bottom = i\n",
    "            break\n",
    "\n",
    "    # 4. Adaptive Thresholding in the table region\n",
    "    table_region = gray[top_boundary:bottom_boundary, :]\n",
    "    thresh_table = cv2.adaptiveThreshold(\n",
    "        table_region, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 3\n",
    "    )\n",
    "\n",
    "    # 5. Hough Lines to find vertical lines\n",
    "    # Increase or decrease threshold=775 if you get too few/many lines\n",
    "    lines = cv2.HoughLinesP(thresh_table, 1, np.pi / 180, threshold=tr1,\n",
    "                            minLineLength=linelength1, maxLineGap=linegap1)\n",
    "    vertical_lines = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            # Check near-vertical\n",
    "            if abs(x2 - x1) < 2:\n",
    "                # Adjust back to full image coordinates\n",
    "                vertical_lines.append((x1, y1 + top_boundary, x2, y2 + top_boundary))\n",
    "\n",
    "    # 6. Draw lines on the OpenCV image\n",
    "    # A) Draw the vertical lines from header_bottom to bottom_boundary\n",
    "    for x1, y1, x2, y2 in vertical_lines:\n",
    "        cv2.line(img, (x1, 350), (x2, bottom_boundary-185), (100, 100, 100), 1)\n",
    "   \n",
    "    print(header_bottom)\n",
    "    # B) Draw horizontal lines at:\n",
    "    #    - top_boundary\n",
    "    #    - bottom_boundary\n",
    "    #    - header_bottom\n",
    "    # We'll draw them across the full width of the image\n",
    "    height, width = img.shape[:2]\n",
    "    # color is (100,100,100); thickness=1 or 2 as you prefer\n",
    "    # Top boundary (green)\n",
    "    cv2.line(img, (0, top_boundary), (width, top_boundary), (0, 255, 0), 2)\n",
    "\n",
    "    # Bottom boundary (green)\n",
    "    cv2.line(img, (0, bottom_boundary), (width, bottom_boundary), (0, 255, 0), 2)\n",
    "\n",
    "    # Header bottom (red)\n",
    "    cv2.line(img, (0, header_bottom), (width, header_bottom), (0, 0, 255), 2)\n",
    "\n",
    "    # 7. Convert back to PIL and save\n",
    "    output_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    output_pil.save(output_path)\n",
    "\n",
    "    print(f\"Saved enhanced table to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "Saved enhanced table to: Lines_W1_boundaries.png\n",
      "405\n",
      "Saved enhanced table to: Lines_W2_boundaries.png\n",
      "407\n",
      "Saved enhanced table to: Lines_W3_boundaries.png\n",
      "410\n",
      "Saved enhanced table to: Lines_W4_boundaries.png\n",
      "402\n",
      "Saved enhanced table to: Lines_W12_boundaries.png\n",
      "407\n",
      "Saved enhanced table to: Lines_W22_boundaries.png\n",
      "408\n",
      "Saved enhanced table to: Lines_W26_boundaries.png\n",
      "427\n",
      "Saved enhanced table to: Lines_W32_boundaries.png\n",
      "430\n",
      "Saved enhanced table to: Lines_W42_boundaries.png\n",
      "399\n",
      "Saved enhanced table to: Lines_W52_boundaries.png\n"
     ]
    }
   ],
   "source": [
    "for week in [1,2,3,4,12,22,26,32,42,52]:\n",
    "    enhance_table_lines_from_pdf_hq(f\"PDFs/W{week}.pdf\", f\"Lines_W{week}_boundaries.png\", page_number=3, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiding text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code works well now! \n",
    "Crops images to the bottom of the table avoding the legend and also places vertical lines at correct positions. 2 of the tables had issues with the green rows being detected but can address that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageColor\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def hex_to_hsv(hex_color):\n",
    "    rgb = ImageColor.getcolor(hex_color, \"RGB\")\n",
    "    r, g, b = [x / 255.0 for x in rgb]\n",
    "    hsv = cv2.cvtColor(\n",
    "        np.uint8([[[b * 255, g * 255, r * 255]]]),\n",
    "        cv2.COLOR_BGR2HSV\n",
    "    )[0][0]\n",
    "    return hsv\n",
    "\n",
    "def enhance_table_lines_from_pdf_hq(\n",
    "    pdf_path, \n",
    "    output_path,\n",
    "    tr1, \n",
    "    linelength1, \n",
    "    linegap1, \n",
    "    toler1, \n",
    "    page_number=0, \n",
    "    dpi=600\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhances vertical column separators and draws horizontal lines at\n",
    "    top boundary, bottom boundary, and header bottom. Uses morphological\n",
    "    filtering to remove small vertical text edges.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF.\n",
    "        output_path (str): Path to save the image (e.g. .png).\n",
    "        tr1 (int): HoughLinesP threshold.\n",
    "        linelength1 (int): HoughLinesP minLineLength.\n",
    "        linegap1 (int): HoughLinesP maxLineGap.\n",
    "        toler1 (int): Tolerance around the HSV hue for detecting green rows.\n",
    "        page_number (int): Which PDF page to process (0-indexed).\n",
    "        dpi (int): Rendering DPI for the PDF page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number]\n",
    "\n",
    "    # 1. Render the PDF page at high DPI\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    img_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    # Convert PIL Image to OpenCV BGR\n",
    "    img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    height1, width1 = img.shape[:2]\n",
    "    total_pixels = height1 * width1\n",
    "    print(\"Total pixels =\", total_pixels)\n",
    "    # 2. Convert to HSV & detect green rows\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    #target_hsv = hex_to_hsv(\"#D8EDCF\")\n",
    "    target_hsv = np.array([102, 12.66, 92.94], dtype=np.uint8)\n",
    "    tolerance = toler1\n",
    "    lower_green = np.array([max(0, target_hsv[0] - tolerance), 50, 50])\n",
    "    upper_green = np.array([min(179, target_hsv[0] + tolerance), 255, 255])\n",
    "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    h_proj_green = np.sum(green_mask, axis=1)\n",
    "    green_row_indices = np.where(h_proj_green > 1000)[0]\n",
    "\n",
    "    if len(green_row_indices) == 0:\n",
    "        print(\"No green rows detected.\")\n",
    "        print(pdf_path)\n",
    "        #return\n",
    "\n",
    "    if len(green_row_indices) == 0:\n",
    "        top_boundary = 800\n",
    "        bottom_boundary = 4500\n",
    "    else:\n",
    "        top_boundary = green_row_indices[0] - 20\n",
    "        bottom_boundary = green_row_indices[-1] + 20\n",
    "    \n",
    "    # 3. Header Row Detection (just above top_boundary)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    header_region = gray[:top_boundary, :]\n",
    "\n",
    "    _, binary_header = cv2.threshold(\n",
    "        header_region, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU\n",
    "    )\n",
    "    h_proj_header = np.sum(binary_header, axis=1)\n",
    "\n",
    "    header_bottom = 0\n",
    "    for i in range(len(h_proj_header) - 1, 0, -1):\n",
    "        if h_proj_header[i] > 40:\n",
    "            header_bottom = i\n",
    "            break\n",
    "\n",
    "    # 4. Adaptive Thresholding in the table region\n",
    "    table_region = gray[top_boundary:bottom_boundary, :]\n",
    "    thresh_table = cv2.adaptiveThreshold(\n",
    "        table_region,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        11,\n",
    "        3\n",
    "    )\n",
    "\n",
    "    #kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 40))\n",
    "    #cleaned = cv2.morphologyEx(thresh_table, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # 5. Hough Lines to find vertical lines (using the filtered 'cleaned' image)\n",
    "    lines = cv2.HoughLinesP(\n",
    "        thresh_table,\n",
    "        1,\n",
    "        np.pi / 180,\n",
    "        threshold=tr1,\n",
    "        minLineLength=linelength1,\n",
    "        maxLineGap=linegap1\n",
    "    )\n",
    "    vertical_lines = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            if abs(x2 - x1) < 5:\n",
    "                vertical_lines.append((x1, y1 + top_boundary, x2, y2 + top_boundary))\n",
    "\n",
    "    # 6. Draw vertical lines on the image\n",
    "    for x1, y1, x2, y2 in vertical_lines:\n",
    "        cv2.line(img, (x1, top_boundary-20), (x2, bottom_boundary+20), (100, 100, 100), 2)\n",
    "\n",
    "    #print(f\"Header bottom: {top_boundary}\")\n",
    "\n",
    "    # Crop the image so that it ends at bottom_boundary + 40\n",
    "    crop_bottom = bottom_boundary + 40\n",
    "    crop_bottom = min(crop_bottom, img.shape[0])\n",
    "    img_cropped = img[:crop_bottom, :]\n",
    "\n",
    "    # 7. Convert back to PIL and save\n",
    "    output_pil = Image.fromarray(cv2.cvtColor(img_cropped, cv2.COLOR_BGR2RGB))\n",
    "    output_pil.save(output_path)\n",
    "\n",
    "    #print(f\"Saved enhanced table to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels = 34799360\n"
     ]
    }
   ],
   "source": [
    "pdf = \"Nigeria_14_Oct_24_W42.pdf\"\n",
    "input_pdf = os.path.join(\"PDFs\", pdf)\n",
    "output_img = os.path.join(\"PDFs_Lines\", f\"Lines_{pdf.replace('.pdf','')}_page3.png\")\n",
    "enhance_table_lines_from_pdf_hq(input_pdf, output_img, tr1=1000, linelength1=70,linegap1=70, toler1 = 10, page_number=3, dpi=600) #length 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels = 34806376\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34757631\n",
      "Total pixels = 34806376\n",
      "No green rows detected.\n",
      "PDFs/Nigeria_01_Aug_24_W31.pdf\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34799360\n",
      "Total pixels = 34806376\n",
      "No green rows detected.\n",
      "PDFs/Nigeria_28_Oct_24_W44.pdf\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n",
      "Total pixels = 34806376\n"
     ]
    }
   ],
   "source": [
    "for pdf in sorted_pdfs:\n",
    "    input_pdf = os.path.join(\"PDFs\", pdf)\n",
    "    output_img = os.path.join(\"PDFs_Lines\", f\"Lines_{pdf.replace('.pdf','')}_page3.png\")\n",
    "    enhance_table_lines_from_pdf_hq(input_pdf, output_img, tr1=1400, linelength1=79,linegap1=50, toler1 = 10, page_number=3, dpi=600) #length 40 was decent\n",
    "\n",
    "    # For DPI 300 tr1 = 850 was good, linelength = 1700, linegap = 100, toler =  10\n",
    "    # But need to find better one for DPI 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15% left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageColor\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def hex_to_hsv(hex_color):\n",
    "    rgb = ImageColor.getcolor(hex_color, \"RGB\")\n",
    "    r, g, b = [x / 255.0 for x in rgb]\n",
    "    hsv = cv2.cvtColor(\n",
    "        np.uint8([[[b * 255, g * 255, r * 255]]]),\n",
    "        cv2.COLOR_BGR2HSV\n",
    "    )[0][0]\n",
    "    return hsv\n",
    "\n",
    "def enhance_table_lines_from_pdf_hq(pdf_path, output_path, page_number=0, dpi=300):\n",
    "    \"\"\"\n",
    "    Enhances vertical column separators and draws horizontal lines at\n",
    "    top boundary, bottom boundary, and header bottom.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF.\n",
    "        output_path (str): Path to save the image (use .png).\n",
    "        page_number (int): Page to process (0-indexed).\n",
    "        dpi (int): Resolution for rendering the PDF page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number]\n",
    "\n",
    "    # 1. Render the PDF page at high DPI\n",
    "    pix = page.get_pixmap(dpi=dpi) \n",
    "    img_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    # Convert PIL Image to OpenCV BGR\n",
    "    img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 2. Convert to HSV & detect green rows\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    height, width = img.shape[:2]\n",
    "    \n",
    "    # We'll only check the left 15% of the page's width\n",
    "    left_15_width = int(width * 0.15)\n",
    "\n",
    "    target_hsv = hex_to_hsv(\"#D8EDCF\")\n",
    "    tolerance = 15\n",
    "    lower_green = np.array([max(0, target_hsv[0] - tolerance), 50, 50])\n",
    "    upper_green = np.array([min(179, target_hsv[0] + tolerance), 255, 255])\n",
    "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "    # Sum only the left 15% columns\n",
    "    green_mask_left = green_mask[:, :left_15_width]\n",
    "    h_proj_green = np.sum(green_mask_left, axis=1)\n",
    "    green_row_indices = np.where(h_proj_green > 0)[0]\n",
    "\n",
    "    if len(green_row_indices) == 0:\n",
    "        print(\"No green rows detected.\")\n",
    "        return\n",
    "\n",
    "    top_boundary = green_row_indices[0]\n",
    "    bottom_boundary = green_row_indices[-1]\n",
    "\n",
    "    # 3. Header Row Detection (just above top_boundary)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    header_region = gray[:top_boundary, :]\n",
    "\n",
    "    # Use Otsu’s threshold for the header\n",
    "    _, binary_header = cv2.threshold(\n",
    "        header_region, 0, 255,\n",
    "        cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU\n",
    "    )\n",
    "    h_proj_header = np.sum(binary_header, axis=1)\n",
    "\n",
    "    header_bottom = 0\n",
    "    for i in range(len(h_proj_header) - 1, 0, -1):\n",
    "        if h_proj_header[i] > 40:\n",
    "            header_bottom = i\n",
    "            break\n",
    "\n",
    "    # 4. Adaptive Thresholding in the table region\n",
    "    table_region = gray[top_boundary:bottom_boundary, :]\n",
    "    thresh_table = cv2.adaptiveThreshold(\n",
    "        table_region,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        11,\n",
    "        3\n",
    "    )\n",
    "\n",
    "    # 5. Hough Lines to find vertical lines\n",
    "    lines = cv2.HoughLinesP(thresh_table, 1, np.pi / 180,\n",
    "                            threshold=775, minLineLength=10, maxLineGap=8)\n",
    "    vertical_lines = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            if abs(x2 - x1) < 5:  # near-vertical\n",
    "                # Adjust back to full image coordinates\n",
    "                vertical_lines.append((x1, y1 + top_boundary, x2, y2 + top_boundary))\n",
    "\n",
    "    # 6. Draw lines on the OpenCV image\n",
    "    # A) Draw the vertical lines from header_bottom to bottom_boundary\n",
    "    for x1, y1, x2, y2 in vertical_lines:\n",
    "        cv2.line(img, (x1, header_bottom), (x2, bottom_boundary), (100, 100, 100), 1)\n",
    "\n",
    "    # B) Draw horizontal lines at top, bottom, and header_bottom\n",
    "    cv2.line(img, (0, top_boundary), (width, top_boundary), (0, 255, 0), 2)     # green\n",
    "    cv2.line(img, (0, bottom_boundary), (width, bottom_boundary), (0, 255, 0), 2) # green\n",
    "    cv2.line(img, (0, header_bottom), (width, header_bottom), (0, 0, 255), 2)  # red\n",
    "\n",
    "    # 7. Convert back to PIL and save\n",
    "    output_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    output_pil.save(output_path)\n",
    "\n",
    "    print(f\"Saved enhanced table to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enhanced table to: Lines_W1_boundaries.png\n",
      "Saved enhanced table to: Lines_W2_boundaries.png\n",
      "Saved enhanced table to: Lines_W3_boundaries.png\n",
      "Saved enhanced table to: Lines_W4_boundaries.png\n",
      "Saved enhanced table to: Lines_W12_boundaries.png\n",
      "Saved enhanced table to: Lines_W22_boundaries.png\n",
      "Saved enhanced table to: Lines_W26_boundaries.png\n",
      "Saved enhanced table to: Lines_W32_boundaries.png\n",
      "Saved enhanced table to: Lines_W42_boundaries.png\n",
      "Saved enhanced table to: Lines_W52_boundaries.png\n"
     ]
    }
   ],
   "source": [
    "for week in [1,2,3,4,12,22,26,32,42,52]:\n",
    "    enhance_table_lines_from_pdf_hq(f\"PDFs/W{week}.pdf\", f\"Lines_W{week}_boundaries.png\", page_number=3, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def rename_lassa_files(folder_path):\n",
    "    \"\"\"\n",
    "    Renames 'An_update_of_Lassa_fever_outbreak_in_Nigeria_041124_45.pdf'\n",
    "    to 'Nigeria_04_Nov_24_W45.pdf', extracting day=04, month=11 => 'Nov', year=24,\n",
    "    and the week number 45.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder that contains the PDF files.\n",
    "    \"\"\"\n",
    "    # For mapping month number to short name\n",
    "    month_map = {\n",
    "        \"01\": \"Jan\", \"02\": \"Feb\", \"03\": \"Mar\", \"04\": \"Apr\",\n",
    "        \"05\": \"May\", \"06\": \"Jun\", \"07\": \"Jul\", \"08\": \"Aug\",\n",
    "        \"09\": \"Sep\", \"10\": \"Oct\", \"11\": \"Nov\", \"12\": \"Dec\",\n",
    "    }\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    for file_path in folder.iterdir():\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        if not file_path.suffix.lower() == \".pdf\":\n",
    "            continue\n",
    "        \n",
    "        old_name = file_path.name\n",
    "        # Example old_name: \"An_update_of_Lassa_fever_outbreak_in_Nigeria_041124_45.pdf\"\n",
    "        \n",
    "        # 1) Split on underscores\n",
    "        parts = old_name.split(\"_\")\n",
    "        # e.g. [\"An\",\"update\",\"of\",\"Lassa\",\"fever\",\"outbreak\",\"in\",\"Nigeria\",\"041124\",\"45.pdf\"]\n",
    "        \n",
    "        if len(parts) < 9:\n",
    "            # If the file name doesn't match the expected pattern, skip it\n",
    "            print(f\"Skipping file (unrecognized pattern): {old_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 2) The date chunk is parts[8] like \"041124\"\n",
    "        date_str = parts[8]  # \"041124\"\n",
    "        \n",
    "        # 3) The week chunk is in parts[9], but includes \".pdf\" at the end, e.g. \"45.pdf\"\n",
    "        week_str_pdf = parts[9]  # \"45.pdf\"\n",
    "        # Remove \".pdf\" from the end\n",
    "        if week_str_pdf.endswith(\".pdf\"):\n",
    "            week_str = week_str_pdf.replace(\".pdf\", \"\")\n",
    "        else:\n",
    "            print(f\"Skipping file (no .pdf in last part): {old_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 4) date_str should be 6 characters: DDMMYY\n",
    "        if len(date_str) != 6:\n",
    "            print(f\"Skipping file (date string not 6 chars): {old_name}\")\n",
    "            continue\n",
    "        dd = date_str[0:2]   # \"04\"\n",
    "        mm = date_str[2:4]   # \"11\"\n",
    "        yy = date_str[4:6]   # \"24\"\n",
    "        \n",
    "        # 5) Convert mm => month name\n",
    "        month_name = month_map.get(mm, \"???\" )  # fallback \"???\"\n",
    "        \n",
    "        # 6) Build new name\n",
    "        # e.g. \"Nigeria_04_Nov_24_W45.pdf\"\n",
    "        new_name = f\"Nigeria_{dd}_{month_name}_{yy}_W{week_str}.pdf\"\n",
    "        \n",
    "        new_path = folder / new_name\n",
    "        # 7) Rename the file\n",
    "        print(f\"Renaming:\\n  {old_name}\\n-> {new_name}\\n\")\n",
    "        file_path.rename(new_path)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    rename_lassa_files(\"PDFs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
