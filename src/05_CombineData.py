# src/05_CombineData.py
"""
Combines yearly extracted Lassa fever data CSVs into a single master CSV file.

This script reads individual CSV files generated by 04_TableExtractionSorting.py.
It expects these files to be located in year-specific directories named
'CSV_LF_{YY}_Sorted' within the 'data/processed/' directory (e.g.,
data/processed/CSV_LF_22_Sorted/). It concatenates data for a specified range
of years from all CSVs found within these directories into a single output file.

Inputs:
- data/processed/CSV_LF_{YY}_Sorted/*.csv (for each year in the range)

Outputs:
- data/processed/combined_lassa_data_{start_year}-{end_year}.csv
"""

import pandas as pd
from pathlib import Path
import logging
import sys
import time

# --- Configuration ---
START_YEAR_SHORT = 21 # Use short year format (YY) for directory names
END_YEAR_SHORT = 25   # Adjust if needed based on available data or current year
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
LOG_DIR = BASE_DIR / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

# --- Add utils to sys.path for logging_config ---
UTILS_DIR = BASE_DIR / "src" / "utils"
if str(UTILS_DIR.parent) not in sys.path:
    sys.path.insert(0, str(UTILS_DIR.parent)) # Add 'src' directory

try:
    # Attempt import assuming script is run from root or src
    from utils.logging_config import configure_logging
except ImportError:
    try:
        # Attempt import assuming script is run from src/utils
        from src.utils.logging_config import configure_logging
    except ImportError:
        print("Error: Could not import configure_logging. Ensure utils/logging_config.py exists "
              "and script is run from a standard location (root, src, or src/utils).", file=sys.stderr)
        # Fallback basic logging if import fails
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        # Define a dummy configure_logging if import failed, to avoid NameError later
        def configure_logging():
            pass


# --- Main Function ---
def combine_yearly_data(processed_base_dir: Path, output_file: Path, start_yy: int, end_yy: int):
    """
    Finds, reads, and combines yearly data CSVs from subdirectories into a single file.

    Args:
        processed_base_dir (Path): Base directory containing the year-specific subdirectories
                                    (e.g., 'data/processed/').
        output_file (Path): Path to save the combined CSV file.
        start_yy (int): The first year (in YY format) to include.
        end_yy (int): The last year (in YY format) to include.
    """
    logger = logging.getLogger(__name__)
    start_year_full = 2000 + start_yy
    end_year_full = 2000 + end_yy
    logger.info(f"Starting data combination process for years {start_year_full}-{end_year_full}.")
    start_time = time.time()

    all_dataframes = []
    years_processed = []
    files_read_count = 0

    for year_short in range(start_yy, end_yy + 1):
        year_dir_name = f"CSV_LF_{year_short}_Sorted"
        year_dir_path = processed_base_dir / year_dir_name
        full_year = 2000 + year_short

        if year_dir_path.is_dir():
            logger.info(f"Scanning directory for year {full_year}: {year_dir_path}")
            year_files = list(year_dir_path.glob('*.csv'))

            if not year_files:
                logger.warning(f"No CSV files found in {year_dir_path}. Skipping year {full_year}.")
                continue

            yearly_dataframes = []
            for file_path in year_files:
                try:
                    logger.debug(f"Reading file: {file_path.name}")
                    df = pd.read_csv(file_path)
                    if not df.empty:
                        # Verify Year and Week columns exist
                        if 'Year' not in df.columns or 'Week' not in df.columns:
                            logger.warning(f"Missing Year or Week column in {file_path.name}, skipping")
                            continue
                        yearly_dataframes.append(df)
                        files_read_count += 1
                    else:
                        logger.warning(f"File is empty, skipping: {file_path.name}")
                except pd.errors.EmptyDataError:
                    logger.warning(f"File is empty (EmptyDataError), skipping: {file_path.name}")
                except Exception as e:
                    logger.error(f"Error reading file {file_path.name}: {e}")

            if yearly_dataframes:
                # Combine all dataframes for the current year
                year_combined_df = pd.concat(yearly_dataframes, ignore_index=True)
                all_dataframes.append(year_combined_df)
                years_processed.append(str(full_year))
                logger.info(f"Successfully read {len(yearly_dataframes)} files for year {full_year}.")
            else:
                 logger.warning(f"No valid data read from CSV files for year {full_year}.")

        else:
            logger.warning(f"Directory for year {full_year} not found: {year_dir_path}. Skipping.")

    if not all_dataframes:
        logger.error("No data files found or read successfully in the specified range. Cannot combine.")
        return

    logger.info(f"Combining data for years: {', '.join(years_processed)}")
    try:
        # Combine data from all processed years
        combined_df = pd.concat(all_dataframes, ignore_index=True)
        logger.info(f"Successfully combined {len(combined_df)} rows from {files_read_count} files across {len(all_dataframes)} years.")

        # Optional: Sort the combined data if columns exist
        sort_columns = []
        if 'Year' in combined_df.columns: sort_columns.append('Year')
        if 'Week' in combined_df.columns: sort_columns.append('Week')
        if 'State' in combined_df.columns: sort_columns.append('State') # Assuming 'State' might exist

        if sort_columns:
            # Ensure numeric columns are treated as numbers for sorting
            if 'Year' in sort_columns: combined_df['Year'] = pd.to_numeric(combined_df['Year'], errors='coerce')
            if 'Week' in sort_columns: combined_df['Week'] = pd.to_numeric(combined_df['Week'], errors='coerce')
            try:
                combined_df.sort_values(by=sort_columns, inplace=True, na_position='last')
                logger.info(f"Sorted combined data by: {', '.join(sort_columns)}")
            except Exception as e:
                logger.warning(f"Could not sort data by {sort_columns}: {e}. Proceeding without sorting.")


        # Convert numeric columns to integers before saving
        integer_columns = ['Suspected', 'Confirmed', 'Probable', 'HCW.', 'Deaths..Confirmed.Cases.']
        for col in integer_columns:
            if col in combined_df.columns:
                # First convert to numeric (handling any non-numeric values)
                combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')
                # Then convert to integer (NaN values will become NaN again)
                combined_df[col] = combined_df[col].fillna(-1).astype(int).replace(-1, pd.NA)
                logger.info(f"Converted column '{col}' to integer type")
            else:
                logger.debug(f"Column '{col}' not found in the combined data")
                
        combined_df.to_csv(output_file, index=False)
        logger.info(f"Combined data saved to {output_file}")

    except Exception as e:
        logger.error(f"Error during final concatenation or saving the combined file: {e}")
        logger.exception("Detailed traceback:") # Add exception details to log

    end_time = time.time()
    logger.info(f"Data combination finished in {end_time - start_time:.2f} seconds.")


# --- Execution Block ---
if __name__ == "__main__":
    # Configure logging using the shared configuration
    configure_logging()
    logger = logging.getLogger(__name__)

    # Define the output filename dynamically based on the year range
    start_year_full = 2000 + START_YEAR_SHORT
    end_year_full = 2000 + END_YEAR_SHORT
    output_filename = f"combined_lassa_data_{start_year_full}-{end_year_full}.csv"
    OUTPUT_FILE_PATH = PROCESSED_DATA_DIR / output_filename
    OUTPUT_FILE_PATH.parent.mkdir(parents=True, exist_ok=True) # Ensure output dir exists

    combine_yearly_data(
        processed_base_dir=PROCESSED_DATA_DIR,
        output_file=OUTPUT_FILE_PATH,
        start_yy=START_YEAR_SHORT,
        end_yy=END_YEAR_SHORT
    )
